{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TONG CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from joblib import dump, load\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "\n",
    "import librosa\n",
    "from librosa import display\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is all sorts of messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self):\n",
    "        self.X = None\n",
    "        self.X_mfcc = None\n",
    "        self.X_mfcc_pca = None\n",
    "        self.X_mfcc_random_crop = None\n",
    "        self.X_mfcc_fixed_crop = None\n",
    "        self.X_chroma = None\n",
    "        self.X_chroma_pca = None\n",
    "        self.pca_mfcc = None\n",
    "        self.pca_chroma = None\n",
    "        self.Y = None\n",
    "        \n",
    "    def save_raw(self, genres=all_genres, songs=num_songs):\n",
    "        assert(self.X is None and self.Y is None)\n",
    "        X, Y = None, None\n",
    "        for g_idx, g in enumerate(genres):\n",
    "            for s_idx in range(songs):\n",
    "                y, sr = librosa.load(f'genres/{g}/{g}.000{s_idx:02d}.au')\n",
    "                y = y[:Y_LIMIT]\n",
    "                if X is None:\n",
    "                    X = y.reshape(1, y.shape[0])\n",
    "                    Y = np.array([[g_idx]])\n",
    "                else:\n",
    "                    X = np.vstack([X, y])\n",
    "                    Y = np.vstack([Y, np.array([[g_idx]])])\n",
    "        Y = Y.ravel()\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        np.savetxt('data/X.csv', X)\n",
    "        np.savetxt('data/Y.csv', Y)\n",
    "        \n",
    "    def save_mfcc(self, genres=all_genres, songs=num_songs):\n",
    "        assert(self.X_mfcc is None)\n",
    "        X_mfcc = None\n",
    "        for g_idx, g in enumerate(genres):\n",
    "            for s_idx in range(songs):\n",
    "                y, sr = librosa.load(f'genres/{g}/{g}.000{s_idx:02d}.au')\n",
    "                y = y[:Y_LIMIT]\n",
    "                mfcc = librosa.feature.mfcc(y, sr=sr, hop_length=512, n_mfcc=13).flatten()\n",
    "                if X_mfcc is None:\n",
    "                    X_mfcc = mfcc.reshape(1, mfcc.shape[0])\n",
    "                else:\n",
    "                    X_mfcc = np.vstack([X_mfcc, mfcc])\n",
    "        self.X_mfcc = X_mfcc\n",
    "        np.savetxt('data/X_mfcc.csv', X_mfcc)\n",
    "        \n",
    "    def save_mfcc_pca(self, pca_dims=100):\n",
    "        assert(self.X_mfcc_pca is None and self.pca_mfcc is None)\n",
    "        pca_mfcc = PCA(n_components=pca_dims, random_state=1)\n",
    "        X_mfcc_pca = pca_mfcc.fit_transform(self.X_mfcc)\n",
    "        self.pca_mfcc = pca_mfcc\n",
    "        self.X_mfcc_pca = X_mfcc_pca\n",
    "        np.savetxt(f'data/X_mfcc_pca{pca_dims}.csv', X_mfcc_pca)\n",
    "        dump(pca_mfcc, f'data/pca_mfcc{pca_dims}.PCA') \n",
    "    \n",
    "    def save_mfcc_random_crop(self):\n",
    "        X_mfcc_crop = None\n",
    "        for mfcc in self.X_mfcc:\n",
    "            crop = None\n",
    "            SEG = 10\n",
    "            SEG_LENGTH = 129\n",
    "            for j in range(SEG):\n",
    "                random_start = np.random.randint(0, 1290-SEG_LENGTH)\n",
    "                random_seg = np.vstack([mfcc[1290*j+random_start : 1290*j+random_start+SEG_LENGTH] for j in range(13)])\n",
    "                random_seg = random_seg.reshape(1, random_seg.shape[0], random_seg.shape[1])\n",
    "                if crop is None:\n",
    "                    crop = random_seg\n",
    "                else:\n",
    "                    crop = np.vstack([crop, random_seg])\n",
    "            if X_mfcc_crop is None:\n",
    "                X_mfcc_crop = crop\n",
    "            else:\n",
    "                X_mfcc_crop = np.vstack([X_mfcc_crop, crop])\n",
    "        self.X_mfcc_random_crop = X_mfcc_crop\n",
    "        np.savetxt('data/X_mfcc_random_crop.csv', X_mfcc_crop.reshape(10000, 1677))\n",
    "    \n",
    "    def save_mfcc_fixed_crop(self):\n",
    "        '''\n",
    "        Evenly divides each song into 10 segments,\n",
    "        producing a 10000 by 13 by 129 array of MFCC coefficients for the segments.\n",
    "        Reshapes into 10000*1677 in order to save as a CSV.\n",
    "        '''\n",
    "        X_mfcc_crop = None\n",
    "        for mfcc in self.X_mfcc:\n",
    "            SEG = 10\n",
    "            SEG_LENGTH = int(1290/SEG)\n",
    "            i = 0\n",
    "            crop = np.stack([np.vstack([mfcc[1290*j+SEG_LENGTH*i : 1290*j+SEG_LENGTH*(i+1)] for j in range(13)]) for i in range(SEG)], axis=0)\n",
    "            if X_mfcc_crop is None:\n",
    "                X_mfcc_crop = crop\n",
    "            else:\n",
    "                X_mfcc_crop = np.vstack([X_mfcc_crop, crop])\n",
    "        self.X_mfcc_fixed_crop = X_mfcc_crop\n",
    "        np.savetxt('data/X_mfcc_fixed_crop.csv', X_mfcc_crop.reshape(10000, 1677))\n",
    "    \n",
    "    def save_chroma(self, genres=all_genres, songs=num_songs):\n",
    "        assert(self.X_chroma is None)\n",
    "        X_chroma = None\n",
    "        for g_idx, g in enumerate(genres):\n",
    "            for s_idx in range(songs):\n",
    "                y, sr = librosa.load(f'genres/{g}/{g}.000{s_idx:02d}.au')\n",
    "                y = y[:Y_LIMIT]\n",
    "                chroma = librosa.feature.chroma_cqt(y, sr=sr, hop_length=512).flatten()\n",
    "                if X_chroma is None:\n",
    "                    X_chroma = chroma.reshape(1, chroma.shape[0])\n",
    "                else:\n",
    "                    X_chroma = np.vstack([X_chroma, chroma])\n",
    "        self.X_chroma = X_chroma\n",
    "        np.savetxt('data/X_chroma.csv', X_chroma)\n",
    "    \n",
    "    def save_chroma_pca(self, pca_dims=100):\n",
    "        assert(self.X_chroma_pca is None and self.pca_chroma is None)\n",
    "        pca_chroma = PCA(n_components=pca_dims)\n",
    "        X_chroma_pca = pca_chroma.fit_transform(self.X_chroma, random_state=1)\n",
    "        self.pca_chroma = pca_chroma\n",
    "        self.X_chroma_pca = X_chroma_pca\n",
    "        np.savetxt(f'data/X_chroma_pca{pca_dims}.csv', X_chroma_pca)\n",
    "        dump(pca_chroma, f'data/pca_chroma{pca_dims}.PCA') \n",
    "    \n",
    "    def load_raw(self):\n",
    "        self.X_raw = np.loadtxt('data/X.csv')\n",
    "        \n",
    "    def load_mfcc(self):\n",
    "        self.X_mfcc = np.loadtxt('data/X_mfcc.csv')\n",
    "    \n",
    "    def load_mfcc_random_crop(self):\n",
    "        self.X_mfcc_random_crop = np.loadtxt('data/X_mfcc_random_crop.csv').reshape(10000, 13, 129)\n",
    "        \n",
    "    def load_mfcc_fixed_crop(self):\n",
    "        self.X_mfcc_fixed_crop = np.loadtxt('data/X_mfcc_fixed_crop.csv').reshape(10000, 13, 129)\n",
    "        \n",
    "    def load_mfcc(self):\n",
    "        self.X_mfcc = np.loadtxt('data/X_mfcc.csv')\n",
    "        \n",
    "    def load_mfcc_pca(self, pca_dims=100):\n",
    "        self.X_mfcc_pca = np.loadtxt(f'data/X_mfcc_pca{pca_dims}.csv')\n",
    "        self.pca_mfcc = load(f'data/pca_mfcc{pca_dims}.PCA')\n",
    "        \n",
    "    def load_chroma(self):\n",
    "        self.X_chroma = np.loadtxt('data/X_chroma.csv')\n",
    "    \n",
    "    def load_chroma_pca(self, pca_dims=100):\n",
    "        self.X_chroma_pca = np.loadtxt(f'data/X_chroma_pca{pca_dims}.csv')\n",
    "        self.pca_chroma = load(f'data/pca_chroma{pca_dims}.PCA') \n",
    "    \n",
    "    def load_Y(self):\n",
    "        self.Y = np.loadtxt('data/Y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only need to run this ONCE!\n",
    "# # Saves features and pca objects to data/...\n",
    "dl = DataLoader()\n",
    "# dl.save_raw(genres=all_genres, songs=100)\n",
    "dl.save_mfcc()\n",
    "# dl.save_mfcc_pca()\n",
    "# dl.save_chroma()\n",
    "# dl.save_chroma_pca()\n",
    "# dl.save_mfcc_fixed_crop()\n",
    "# dl.save_mfcc_random_crop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 13, 1290)\n"
     ]
    }
   ],
   "source": [
    "# Load from CSVs instead of saving\n",
    "dl = DataLoader()\n",
    "dl.load_mfcc()\n",
    "dl.X_mfcc = dl.X_mfcc.reshape((1000,13,-1))\n",
    "print(dl.X_mfcc.shape)\n",
    "# dl.load_mfcc_pca()\n",
    "# dl.load_mfcc_random_crop()\n",
    "# dl.load_mfcc_fixed_crop()\n",
    "# dl.load_chroma()\n",
    "# dl.load_chroma_pca()\n",
    "# dl.load_Y()\n",
    "# print(dl.X_mfcc.shape, dl.X_mfcc_pca.shape, dl.X_chroma.shape, dl.X_chroma_pca.shape, dl.Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 1290)\n"
     ]
    }
   ],
   "source": [
    "X_mfcc = None\n",
    "for g_idx, g in enumerate(all_genres):\n",
    "    for s_idx in range(100):\n",
    "        y, sr = librosa.load(f'genres/{g}/{g}.000{s_idx:02d}.au')\n",
    "        y = y[:Y_LIMIT]\n",
    "        mfcc = librosa.feature.mfcc(y, sr=sr, hop_length=512, n_mfcc=13).shape\n",
    "        print(mfcc)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dunno about this just like copy paste this in later I guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making torch style dataset and dataloader\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class MusicDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.Y = torch.from_numpy(Y)\n",
    "\n",
    "    def __len__(self):\n",
    "        assert(self.X.size()[0] == self.Y.size()[0])\n",
    "        return self.X.size()[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.X[index,].reshape((1,self.X.size()[1]))\n",
    "        \n",
    "        # normalizing\n",
    "        data = (data - data.mean())/data.std()\n",
    "        \n",
    "        label = self.Y[index]\n",
    "        return (data, label)\n",
    "    \n",
    "trainset = MusicDataset(X_train, Y_train)\n",
    "testset = MusicDataset(X_test, Y_test)\n",
    "\n",
    "print(len(trainset))\n",
    "print(len(testset))\n",
    "\n",
    "genre_trainloader = data.DataLoader(trainset, batch_size = 64, shuffle = True)\n",
    "genre_testloader = data.DataLoader(testset, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torchsummary import summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCNN + RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the idea is that the input is 750 x 1 x 13 x 1290\n",
    "# or more generally, N x Cin x Feat x Time\n",
    "# don't forget to unsqueeze\n",
    "\n",
    "\n",
    "# X is 750 x 1 x 13 x 1290\n",
    "\n",
    "params = {\n",
    "#     'conv_in_size' = tuple(X.size)\n",
    "    'cnn_in_size' = (750, 1, 13, 1290)\n",
    "    'cnn_out_channels' = 10\n",
    "    'cnn_kernel' = 7\n",
    "    'rnn_in_size' = 5\n",
    "}\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        num_features = params['cnn_in_size'][2]\n",
    "        out_channels = params['cnn_out_channels']\n",
    "        ker = params['cnn_kernel']\n",
    "        out_features = params['rnn_in_size']\n",
    "        \n",
    "        out_size = num_features - ker[0] + 1\n",
    "        \n",
    "        self.conv = nn.Conv2d(1, out_channels, ker)\n",
    "        self.fc = nn.Linear(out_channels*out_size, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        # out.size() = N x out_channels x out_size x time\n",
    "        out = out.permute(0,3,1,2)\n",
    "        # out.size() = N x time x out_channels x out_size\n",
    "        out = out.view(out.size()[0], out.size()[1], -1)\n",
    "        # out.size() = N x time x out_channels*out_size\n",
    "        out = self.fc(out)\n",
    "        # out.size() = N x time x out_features\n",
    "        out = F.relu(out)\n",
    "        out = out.permute(1, 0, 2)\n",
    "        # out.size() = time x N x out_features\n",
    "        return out\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Conv_Block(nn.Module):\n",
    "    \n",
    "#     def __init__(self, in_planes, planes, kernel_size, stride, pool_size):\n",
    "        \n",
    "#         super(Conv_Block, self).__init__()\n",
    "        \n",
    "#         self.conv = nn.Conv1d(in_channels = in_planes, out_channels = planes, kernel_size = kernel_size, stride=stride)\n",
    "#         self.dropout = nn.Dropout(p=0.5)\n",
    "#         self.maxpool = nn.AvgPool1d(kernel_size = pool_size)\n",
    "#         self.bnorm = nn.BatchNorm1d(num_features = planes)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.maxpool(x)\n",
    "#         x = self.bnorm(x)\n",
    "#         x = F.relu(x)\n",
    "        \n",
    "#         return x\n",
    "\n",
    "# class Conv(nn.Module):\n",
    "    \n",
    "#     def __init__(self, params):    \n",
    "#         super(Conv, self).__init__()\n",
    "        \n",
    "#         filters = params['filters']\n",
    "#         kernel_sizes = params['kernel_sizes']\n",
    "#         strides = params['strides']\n",
    "#         avg_pool_sizes = params['max_pool_sizes']\n",
    "        \n",
    "#         assert(len(filters) == len(kernel_sizes))\n",
    "#         assert(len(filters) == len(strides))\n",
    "#         assert(len(filters) == len(avg_pool_sizes))\n",
    "        \n",
    "#         prev_outplanes = 1\n",
    "#         prev_outsize = params['input_length']\n",
    "        \n",
    "#         layers = []\n",
    "        \n",
    "#         for i in range(len(filters)):\n",
    "#             inplanes = prev_outplanes\n",
    "#             outplanes = inplanes * filters[i]\n",
    "#             out_size = math.floor((math.floor((prev_outsize - kernel_sizes[i]) / float(strides[i]))+\\\n",
    "#                                 1)/float(avg_pool_sizes[i]))\n",
    "            \n",
    "#             prev_outsize = out_size\n",
    "#             prev_outplanes = outplanes\n",
    "            \n",
    "#             new_block = Conv_Block(inplanes, outplanes, kernel_sizes[i], strides[i], avg_pool_sizes[i])\n",
    "            \n",
    "#             layers.append(new_block)\n",
    "            \n",
    "#         self.convs = nn.Sequential(*layers)\n",
    "        \n",
    "#         in_features = prev_outsize * prev_outplanes\n",
    "        \n",
    "#         self.fc = nn.Linear(in_features = in_features, out_features = 10)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.convs(x)\n",
    "#         x = x.view(x.size()[0], -1)\n",
    "#         x = self.fc(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging\n",
    "\n",
    "# big big convnet\n",
    "# conv_params = {\n",
    "#     'input_length' : 16770,\n",
    "#     'filters' : [8, 8, 8, 8],\n",
    "#     'kernel_sizes' : [64, 32, 16, 8],\n",
    "#     'strides' : [8, 4, 2, 1],\n",
    "#     'max_pool_sizes' : [2, 2, 2, 2]\n",
    "# }\n",
    "\n",
    "# smaller convnet\n",
    "# conv_params = {\n",
    "#     'input_length' : 16770,\n",
    "#     'filters' : [8, 16, 64],\n",
    "#     'kernel_sizes' : [64, 8, 4],\n",
    "#     'strides' : [8, 4, 2],\n",
    "#     'max_pool_sizes' : [16, 4, 2]\n",
    "# }\n",
    "\n",
    "# smallest convnet\n",
    "conv_params = {\n",
    "    'input_length' : 16770,\n",
    "    'filters' : [8, 64],\n",
    "    'kernel_sizes' : [64, 4],\n",
    "    'strides' : [8, 2],\n",
    "    'max_pool_sizes' : [32, 4]\n",
    "}\n",
    "\n",
    "# observation: this net works as long as we don't run out of elements to convolute across (no padding)\n",
    "\n",
    "conv_test = Conv(conv_params)\n",
    "# clipper = WeightClipper()\n",
    "# conv_test.apply(clipper)\n",
    "summary(conv_test, (1,16770))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DConv_Block(nn.Module):\n",
    "    \n",
    "#     def __init__(self, in_planes, planes, kernel_size, dialation, pool_size):\n",
    "        \n",
    "#         super(DConv_Block, self).__init__()\n",
    "        \n",
    "#         padding = int((kernel_size + (kernel_size - 1)*(dialation - 1) - 1)/2)\n",
    "        \n",
    "#         self.dconv = nn.Conv1d(in_channels = in_planes, out_channels = planes,\n",
    "#                                kernel_size = kernel_size, stride=1, padding=padding, dilation=dialation)\n",
    "#         self.dropout = nn.Dropout(p=0.5)\n",
    "#         self.avgpool = nn.AvgPool1d(kernel_size = pool_size)\n",
    "#         self.bnorm = nn.BatchNorm1d(num_features = planes)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.dconv(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = self.bnorm(x)\n",
    "#         x = F.relu(x)\n",
    "#         return x\n",
    "\n",
    "# class DConv(nn.Module):\n",
    "    \n",
    "#     def __init__(self, params):    \n",
    "#         super(DConv, self).__init__()\n",
    "        \n",
    "#         filters = params['filters']\n",
    "#         kernel_sizes = params['kernel_sizes']\n",
    "#         dialations = params['dialations']\n",
    "#         avg_pool_sizes = params['avg_pool_sizes']\n",
    "        \n",
    "#         assert(len(filters) == len(kernel_sizes))\n",
    "#         assert(len(filters) == len(dialations))\n",
    "#         assert(len(filters) == len(avg_pool_sizes))\n",
    "        \n",
    "#         prev_outplanes = 1\n",
    "#         prev_outsize = params['input_length']\n",
    "\n",
    "#         layers = []\n",
    "        \n",
    "#         for i in range(len(filters)):\n",
    "#             inplanes = prev_outplanes\n",
    "#             outplanes = inplanes * filters[i]\n",
    "            \n",
    "#             out_size = math.floor(prev_outsize / float(avg_pool_sizes[i]))\n",
    "            \n",
    "#             prev_outsize = out_size\n",
    "#             prev_outplanes = outplanes\n",
    "            \n",
    "#             new_block = DConv_Block(inplanes, outplanes, kernel_sizes[i], dialations[i], avg_pool_sizes[i])\n",
    "            \n",
    "#             layers.append(new_block)\n",
    "            \n",
    "#         self.dconvs = nn.Sequential(*layers)\n",
    "        \n",
    "#         in_features = prev_outsize * prev_outplanes\n",
    "        \n",
    "#         self.fc = nn.Linear(in_features = in_features, out_features = 10)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.dconvs(x)\n",
    "#         x = x.view(x.size()[0], -1)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging\n",
    "dconv_params = {\n",
    "    'input_length' : 16770,\n",
    "    'filters' : [16, 4],\n",
    "    'kernel_sizes': [64, 16],\n",
    "    'dialations': [8, 2],\n",
    "    'avg_pool_sizes': [32, 4]\n",
    "}\n",
    "\n",
    "# observation: this net works as long as dialation_2 is even\n",
    "\n",
    "dconv_test = DConv(dconv_params)\n",
    "summary(dconv_test, (1,16770))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining training and testing functions\n",
    "\n",
    "def train(net, criterion, optimizer, num_epochs, trainloader):\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        print(\"Epoch: \" + str(epoch+1))\n",
    "        running_loss = 0.0\n",
    "        for data in trainloader:\n",
    "            \n",
    "            inputs, labels = data\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        print(\"Loss: \" + str(running_loss / 750.0) + ' Accuracy: ' + str((100 * correct / total)) + '%')\n",
    "    print('Finished Training')\n",
    "\n",
    "def test(net, testloader):\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    # todo: calculate length of testloader instead of hardcoding\n",
    "            \n",
    "    print('Accuracy of the network on 250 test datapoints: ' + str((100 * correct / total)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from brendan\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(test_labels, predictions, title):\n",
    "    ax= plt.subplot()\n",
    "    cm = confusion_matrix(test_labels, predictions)\n",
    "    sns.heatmap(cm, annot=True, ax = ax, cmap = sns.cm.rocket_r); #annot=True to annotate cells\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "    ax.set_title(f'{title} Confusion Matrix'); \n",
    "    ax.set_ylim(top=0, bottom=10)\n",
    "    ax.xaxis.set_ticklabels(all_genres); ax.yaxis.set_ticklabels(all_genres);\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "    plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_labels = Y_test\n",
    "\n",
    "def net_confusion_matrix(net, testloader, test_labels, title):\n",
    "    \n",
    "    separate_predictions = []\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            \n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            separate_predictions.append(predicted.numpy())\n",
    "    \n",
    "    predictions = np.concatenate(separate_predictions)\n",
    "    plot_confusion_matrix(test_labels, predictions, title)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt at regularization\n",
    "\n",
    "class WeightClipper():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, module):\n",
    "        if hasattr(module, 'weight'):\n",
    "            w = module.weight.data\n",
    "            w = w.clamp(-1,1)\n",
    "\n",
    "clipper = WeightClipper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_net = Conv(conv_params)\n",
    "\n",
    "# regularizing\n",
    "conv_net.apply(clipper)\n",
    "\n",
    "conv_crit = nn.CrossEntropyLoss()\n",
    "conv_opt = torch.optim.Adam(conv_net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dconv_net = DConv(dconv_params)\n",
    "\n",
    "# regularizing\n",
    "dconv_net.apply(clipper)\n",
    "\n",
    "dconv_crit = nn.CrossEntropyLoss()\n",
    "dconv_opt = torch.optim.Adam(dconv_net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # training and evaluating CNN\n",
    "\n",
    "# train(conv_net, conv_crit, conv_opt, 30, genre_trainloader)\n",
    "# test(conv_net, genre_testloader)\n",
    "# net_confusion_matrix(conv_net, genre_testloader, Y_test, \"Preliminary CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # training and evaluating DCNN\n",
    "\n",
    "# train(dconv_net, dconv_crit, dconv_opt, 30, genre_trainloader)\n",
    "# test(dconv_net, genre_testloader)\n",
    "# net_confusion_matrix(dconv_net, genre_testloader, Y_test, \"Preliminary DCNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- try different datapreprocessing: change # of channels for 1d stuff so that we get slices through all the different features of the MFCC \n",
    "- rewrite code for DCNN generation to support more versitile dimensionalities\n",
    "- make RNN: LSTM + (D)CNN\n",
    "- uncomment out bnorm in CNN generation code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
