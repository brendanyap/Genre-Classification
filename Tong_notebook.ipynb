{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TONG CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from joblib import dump, load\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "import librosa\n",
    "from librosa import display\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fecc31183b0>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting seeds\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is all sorts of messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genres = ['blues','classical','country','disco','hiphop','jazz','metal','pop','reggae','rock']\n",
    "num_songs = 100\n",
    "sr = 22050\n",
    "Y_LIMIT = 660000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, seg=5):\n",
    "        '''\n",
    "        Initializes the DataLoader.\n",
    "        \n",
    "        Forces deteriminism by setting np.seed=1.\n",
    "        \n",
    "        self.SEG is the number of fixed-length segments, and the number of random crops to take\n",
    "        self.SEG_LENGTH is the length of each fixed-length segment\n",
    "        self.RANDOM_SEG_LENGTH is the length of each random crop\n",
    "        \n",
    "        self.train_idxs is a list of 750 training indices\n",
    "        self.test_idxs is a list of 250 testing indices\n",
    "        \n",
    "        self.train_crop_idxs is a list of 750*self.SEG training indices, corresponding to the same indices above\n",
    "        self.test_crop_idxs  is a list of 750*self.SEG testing indices,  corresponding to the same indices above\n",
    "        \n",
    "        eg. train_idxs      = [1, 2, 4, ...]\n",
    "            train_crop_idxs = [10 ... 19, 20 ... 29, 40 ... 49, ...]\n",
    "            \n",
    "        Cropped X's retain the order of the uncropped X's\n",
    "        i.e. the first 10 self.mfcc_fixed_crop entries correspond to the first self.mfcc entry.\n",
    "        '''\n",
    "        np.random.seed(1)\n",
    "        self.X_mfcc = None\n",
    "        self.X_mfcc_random_crop = None\n",
    "        self.X_mfcc_fixed_crop = None\n",
    "        \n",
    "        self.X_chroma = None\n",
    "        self.X_chroma_random_crop = None\n",
    "        self.X_chroma_fixed_crop = None\n",
    "        \n",
    "        self.Y = None\n",
    "        self.Y_crop = None\n",
    "        \n",
    "        self.SEG = seg   # Must evenly divide 30\n",
    "        self.SEG_LENGTH = int(1290/self.SEG)\n",
    "        self.RANDOM_SEG_LENGTH = 1200\n",
    "        self.PAD = 1290 - self.RANDOM_SEG_LENGTH\n",
    "        self.RANDOM_STARTS = np.random.randint(low=0, high=1290-self.RANDOM_SEG_LENGTH, size=(1000, self.SEG))\n",
    "        \n",
    "        self.train_idxs = np.sort(np.random.choice(np.array([i for i in range(1000)]), size=500, replace=False))\n",
    "        self.val_idxs = np.sort(np.random.choice(np.array([i for i in range(1000) if i not in self.train_idxs]), size=250, replace=False))\n",
    "        self.test_idxs = np.array([i for i in range(1000) if i not in self.train_idxs and i not in self.val_idxs])\n",
    "\n",
    "        self.train_crop_idxs = np.hstack([np.array([i*self.SEG+j for j in range(self.SEG)]) for i in self.train_idxs])\n",
    "        self.val_crop_idxs = np.hstack([np.array([i*self.SEG+j for j in range(self.SEG)]) for i in self.val_idxs])\n",
    "        self.test_crop_idxs = np.hstack([np.array([i*self.SEG+j for j in range(self.SEG)]) for i in self.test_idxs])\n",
    "\n",
    "    def save_mfcc(self, genres=all_genres, songs=num_songs):\n",
    "        '''\n",
    "        Saves MFCC Coefficients.\n",
    "        Produces a 1000 x 16770 array.\n",
    "        '''\n",
    "        assert(self.X_mfcc is None)\n",
    "        X_mfcc = None\n",
    "        for g_idx, g in enumerate(genres):\n",
    "            for s_idx in range(songs):\n",
    "                y, sr = librosa.load(f'genres/{g}/{g}.000{s_idx:02d}.au')\n",
    "                y = y[:Y_LIMIT]\n",
    "                mfcc = librosa.feature.mfcc(y, sr=sr, hop_length=512, n_mfcc=13).flatten()\n",
    "                if X_mfcc is None:\n",
    "                    X_mfcc = mfcc.reshape(1, mfcc.shape[0])\n",
    "                else:\n",
    "                    X_mfcc = np.vstack([X_mfcc, mfcc])\n",
    "        scaler = StandardScaler()\n",
    "        self.X_mfcc = scaler.fit_transform(X_mfcc)\n",
    "        np.savetxt('data/X_mfcc.csv', self.X_mfcc)\n",
    "    \n",
    "    def save_mfcc_random_crop(self):\n",
    "        '''\n",
    "        Saves self.SEG random crops of MFCC for every original training sample.\n",
    "        \n",
    "        Produces a 10000 x 13 x 1200 array, padded with zeros to 10000 x 13 x 1290.\n",
    "        Reshapes into 10000*16770 for the CSV.\n",
    "        '''\n",
    "        assert(self.X_mfcc_random_crop is None and self.X_mfcc is not None)\n",
    "        X_mfcc_crop = None\n",
    "        for i, mfcc in enumerate(self.X_mfcc):\n",
    "            crop = None\n",
    "            for j in range(self.SEG):\n",
    "                random_start = self.RANDOM_STARTS[i][j]\n",
    "                random_seg = np.vstack([mfcc[1290*k+random_start : 1290*k+random_start+self.RANDOM_SEG_LENGTH] for k in range(13)])\n",
    "                random_seg = np.pad(random_seg, ((0, 0), (0, self.PAD)), 'constant')\n",
    "                random_seg = random_seg.reshape(1, random_seg.shape[0], random_seg.shape[1])\n",
    "                if crop is None:\n",
    "                    crop = random_seg\n",
    "                else:\n",
    "                    crop = np.vstack([crop, random_seg])\n",
    "            if X_mfcc_crop is None:\n",
    "                X_mfcc_crop = crop\n",
    "            else:\n",
    "                X_mfcc_crop = np.vstack([X_mfcc_crop, crop])\n",
    "        self.X_mfcc_random_crop = X_mfcc_crop\n",
    "        np.savetxt(f'data/X_mfcc_random_crop_{self.SEG}.csv', X_mfcc_crop.reshape(1000*self.SEG, 13*1290))\n",
    "    \n",
    "    def save_mfcc_fixed_crop(self):\n",
    "        '''\n",
    "        Saves self.SEG even segments of MFCC for every original training sample.\n",
    "        \n",
    "        Produces a 10000 x 13 x 129 array of MFCC coefficients for the segments.\n",
    "        Reshapes into 10000*1677 for the CSV.\n",
    "        '''\n",
    "        assert(self.X_mfcc_fixed_crop is None and self.X_mfcc is not None)\n",
    "        X_mfcc_crop = None\n",
    "        for mfcc in self.X_mfcc:\n",
    "            crop = np.stack([np.vstack([mfcc[1290*j+self.SEG_LENGTH*i : 1290*j+self.SEG_LENGTH*(i+1)] for j in range(13)]) for i in range(self.SEG)], axis=0)\n",
    "            if X_mfcc_crop is None:\n",
    "                X_mfcc_crop = crop\n",
    "            else:\n",
    "                X_mfcc_crop = np.vstack([X_mfcc_crop, crop])\n",
    "        self.X_mfcc_fixed_crop = X_mfcc_crop\n",
    "        np.savetxt(f'data/X_mfcc_fixed_crop_{self.SEG}.csv', X_mfcc_crop.reshape(1000*self.SEG, 13*self.SEG_LENGTH))\n",
    "    \n",
    "    def save_chroma(self, genres=all_genres, songs=num_songs):\n",
    "        '''\n",
    "        Saves Chromas.\n",
    "        Produces a 1000 x 15480 array.\n",
    "        '''\n",
    "        assert(self.X_chroma is None)\n",
    "        X_chroma = None\n",
    "        for g_idx, g in enumerate(genres):\n",
    "            for s_idx in range(songs):\n",
    "                y, sr = librosa.load(f'genres/{g}/{g}.000{s_idx:02d}.au')\n",
    "                y = y[:Y_LIMIT]\n",
    "                chroma = librosa.feature.chroma_cqt(y, sr=sr, hop_length=512).flatten()\n",
    "                if X_chroma is None:\n",
    "                    X_chroma = chroma.reshape(1, chroma.shape[0])\n",
    "                else:\n",
    "                    X_chroma = np.vstack([X_chroma, chroma])\n",
    "        scaler = StandardScaler()\n",
    "        self.X_chroma = scaler.fit_transform(X_chroma)\n",
    "        np.savetxt('data/X_chroma.csv', self.X_chroma)\n",
    "\n",
    "    def save_chroma_random_crop(self):\n",
    "        '''\n",
    "        Saves 10 random crops of Chromas for every original training sample.\n",
    "        \n",
    "        Produces a 10000 x 12 x 1200 array, padded with zeros to 10000 x 12 x 1290.\n",
    "        Reshapes into 10000*15480 for the CSV.\n",
    "        '''\n",
    "        assert(self.X_chroma_random_crop is None and self.X_chroma is not None)\n",
    "        X_chroma_crop = None\n",
    "        for i, chroma in enumerate(self.X_chroma):\n",
    "            crop = None\n",
    "            for j in range(self.SEG):\n",
    "                random_start = self.RANDOM_STARTS[i][j]\n",
    "                random_seg = np.vstack([chroma[1290*k+random_start : 1290*k+random_start+self.RANDOM_SEG_LENGTH] for k in range(12)])\n",
    "                random_seg = np.pad(random_seg, ((0, 0), (0, self.PAD)), 'constant')\n",
    "                random_seg = random_seg.reshape(1, random_seg.shape[0], random_seg.shape[1])\n",
    "                if crop is None:\n",
    "                    crop = random_seg\n",
    "                else:\n",
    "                    crop = np.vstack([crop, random_seg])\n",
    "            if X_chroma_crop is None:\n",
    "                X_chroma_crop = crop\n",
    "            else:\n",
    "                X_chroma_crop = np.vstack([X_chroma_crop, crop])\n",
    "        self.X_chroma_random_crop = X_chroma_crop\n",
    "        np.savetxt(f'data/X_chroma_random_crop_{self.SEG}.csv', X_chroma_crop.reshape(1000*self.SEG, 12*1290))\n",
    "        \n",
    "    def save_chroma_fixed_crop(self):\n",
    "        '''\n",
    "        Saves 10 even segments of Chromas for every original training sample.\n",
    "        \n",
    "        Produces a 10000 x 12 x 129 array of MFCC coefficients for the segments.\n",
    "        Reshapes into 10000*1548 for the CSV.\n",
    "        '''\n",
    "        assert(self.X_chroma_fixed_crop is None and self.X_chroma is not None)\n",
    "        X_chroma_crop = None\n",
    "        for chroma in self.X_chroma:\n",
    "            crop = np.stack([np.vstack([chroma[1290*j+self.SEG_LENGTH*i : 1290*j+self.SEG_LENGTH*(i+1)] for j in range(12)]) for i in range(self.SEG)], axis=0)\n",
    "            if X_chroma_crop is None:\n",
    "                X_chroma_crop = crop\n",
    "            else:\n",
    "                X_chroma_crop = np.vstack([X_chroma_crop, crop])\n",
    "        self.X_chroma_fixed_crop = X_chroma_crop\n",
    "        np.savetxt(f'data/X_chroma_fixed_crop_{self.SEG}.csv', X_chroma_crop.reshape(1000*self.SEG, 12*self.SEG_LENGTH))\n",
    "    \n",
    "    '''\n",
    "    If X_mfcc has been saved, but we aborted before saving X_mfcc_random_crop (or X_mfcc_fixed_crop), \n",
    "    we can call load_mfcc with tensor=False to load the MFCC in 2D and then call dl.save_random_crop().\n",
    "    Note that all load functions reshape into tensors by default.\n",
    "    '''  \n",
    "    \n",
    "    def load_mfcc(self, tensor=True):\n",
    "        self.X_mfcc = np.loadtxt('data/X_mfcc.csv')\n",
    "        if tensor:\n",
    "            self.X_mfcc = self.X_mfcc.reshape(1000, 13, 1290)\n",
    "    \n",
    "    def load_mfcc_random_crop(self):\n",
    "        self.X_mfcc_random_crop = np.loadtxt(f'data/X_mfcc_random_crop_{self.SEG}.csv').reshape(1000*self.SEG, 13, 1290)\n",
    "        \n",
    "    def load_mfcc_fixed_crop(self):\n",
    "        self.X_mfcc_fixed_crop = np.loadtxt(f'data/X_mfcc_fixed_crop_{self.SEG}.csv').reshape(1000*self.SEG, 13, self.SEG_LENGTH)\n",
    "        \n",
    "    def load_chroma(self, tensor=True):\n",
    "        self.X_chroma = np.loadtxt('data/X_chroma.csv')\n",
    "        if tensor:\n",
    "            self.X_chroma = self.X_chroma.reshape(1000, 12, 1290)\n",
    "    \n",
    "    def load_chroma_random_crop(self):\n",
    "        self.X_chroma_random_crop = np.loadtxt(f'data/X_chroma_random_crop_{self.SEG}.csv').reshape(1000*self.SEG, 12, 1290)\n",
    "        \n",
    "    def load_chroma_fixed_crop(self):\n",
    "        self.X_chroma_fixed_crop = np.loadtxt(f'data/X_chroma_fixed_crop_{self.SEG}.csv').reshape(1000*self.SEG, 12, self.SEG_LENGTH)\n",
    "    \n",
    "    def load_Y(self):\n",
    "        self.Y = np.array([int(i/100) for i in range(1000)]).ravel()\n",
    "        \n",
    "    def load_Y_crop(self):\n",
    "        self.Y_crop = np.array([int(i/(100 * self.SEG)) for i in range(1000 * self.SEG)]).ravel()\n",
    "        \n",
    "    def train_test_val_split(self, data, is_cropped):\n",
    "        '''\n",
    "        Splits an X_data into train, validation, and test sets.\n",
    "        \n",
    "        is_cropped=True for splitting random or fixed crops, iscropped=False for splitting original mfcc/chroma\n",
    "        \n",
    "        Train, val, and test indices are consistent every time train_test_split is called.\n",
    "        \n",
    "        Furthermore, cropped train and test indices are returned so that batches of size self.SEG are together, \n",
    "        with each batch corresponding to a single non-cropped index.\n",
    "        '''\n",
    "        if is_cropped:\n",
    "            return np.take(data, self.train_crop_idxs, 0), np.take(data, self.val_crop_idxs, 0), np.take(data, self.test_crop_idxs, 0)\n",
    "        else:\n",
    "            return np.take(data, self.train_idxs, 0), np.take(data, self.val_idxs, 0), np.take(data, self.test_idxs, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # # Saves features to data/...  (run once!!)\n",
    "# dl = DataLoader(5)\n",
    "# dl.save_mfcc()\n",
    "# dl.save_mfcc_fixed_crop()\n",
    "# dl.save_mfcc_random_crop()\n",
    "# dl.save_chroma()\n",
    "# dl.save_chroma_fixed_crop()\n",
    "# dl.save_chroma_random_crop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 59s, sys: 37.5 s, total: 4min 37s\n",
      "Wall time: 4min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# # Load from CSVs in data/...\n",
    "dl = DataLoader(5)\n",
    "dl.load_mfcc()\n",
    "dl.load_mfcc_fixed_crop()\n",
    "dl.load_mfcc_random_crop()\n",
    "dl.load_chroma()\n",
    "dl.load_chroma_fixed_crop()\n",
    "dl.load_chroma_random_crop()\n",
    "dl.load_Y()\n",
    "dl.load_Y_crop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 13, 1290),\n",
       " (5000, 13, 1290),\n",
       " (5000, 13, 258),\n",
       " (1000, 12, 1290),\n",
       " (5000, 12, 1290),\n",
       " (5000, 12, 258))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.X_mfcc.shape, dl.X_mfcc_random_crop.shape, dl.X_mfcc_fixed_crop.shape, dl.X_chroma.shape, dl.X_chroma_random_crop.shape, dl.X_chroma_fixed_crop.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Test sets for each part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train, validate, and test the vanilla model\n",
      "\n",
      "Train size:    (500, 25, 1290)      Train labels size: (500,)\n",
      "Val size:      (250, 25, 1290)      Val labels size:   (250,)\n",
      "Test size:     (250, 25, 1290)      Test labels size:  (250,)\n"
     ]
    }
   ],
   "source": [
    "# Train on X_train (750),\n",
    "# Test on X_test (250)\n",
    "print('Train, validate, and test the vanilla model\\n')\n",
    "\n",
    "Xm_train, Xm_val, Xm_test = dl.train_test_val_split(dl.X_mfcc, is_cropped=False)\n",
    "Xc_train, Xc_val, Xc_test = dl.train_test_val_split(dl.X_chroma, is_cropped=False)\n",
    "\n",
    "X_train = np.concatenate([Xm_train, Xc_train], axis=1)\n",
    "X_val = np.concatenate([Xm_val, Xc_val], axis=1)\n",
    "X_test = np.concatenate([Xm_test, Xc_test], axis=1)\n",
    "\n",
    "Y_train, Y_val, Y_test = dl.train_test_val_split(dl.Y, is_cropped=False)\n",
    "\n",
    "print(f'Train size:    {X_train.shape}', f'     Train labels size: {Y_train.shape}')\n",
    "print(f'Val size:      {X_val.shape}', f'     Val labels size:   {Y_val.shape}')\n",
    "print(f'Test size:     {X_test.shape}', f'     Test labels size:  {Y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on segments, validate and test by accumulating votes of segments\n",
      "\n",
      "Train size:  (2500, 25, 258)      Train labels size: (2500,)\n",
      "Val size:    (1250, 25, 258)      Val labels size:   (250,)\n",
      "Test size:   (1250, 25, 258)      Test labels size:  (250,)\n"
     ]
    }
   ],
   "source": [
    "# Train on X_mfcc_fixed_crop_train (7500),\n",
    "# Test on X_mfcc_fixed_crop_test (2500),\n",
    "# Aggregate into predictions on X_mfcc_test (250)\n",
    "print('Train on segments, validate and test by accumulating votes of segments\\n')\n",
    "\n",
    "Xmf_train, Xmf_val, Xmf_test = \\\n",
    "    dl.train_test_val_split(dl.X_mfcc_fixed_crop, is_cropped=True)\n",
    "Xcf_train, Xcf_val, Xcf_test = \\\n",
    "    dl.train_test_val_split(dl.X_chroma_fixed_crop, is_cropped=True)\n",
    "\n",
    "Xf_train = np.concatenate([Xmf_train, Xcf_train], axis=1)\n",
    "Xf_val = np.concatenate([Xmf_val, Xcf_val], axis=1)\n",
    "Xf_test = np.concatenate([Xmf_test, Xcf_test], axis=1)\n",
    "\n",
    "Y_crop_train, _, _ = dl.train_test_val_split(dl.Y_crop, is_cropped=True)\n",
    "\n",
    "print(f'Train size:  {Xf_train.shape}', f'     Train labels size: {Y_crop_train.shape}')\n",
    "print(f'Val size:    {Xf_val.shape}', f'     Val labels size:   {Y_val.shape}')\n",
    "print(f'Test size:   {Xf_test.shape}', f'     Test labels size:  {Y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on padded random crops, validate and test on uncropped test\n",
      "\n",
      "Train size:   (2500, 25, 1290)     Train labels size: (2500,)\n",
      "Val size:     (250, 25, 1290)      Val labels size:   (250,)\n",
      "Test size:    (250, 25, 1290)      Test labels size:  (250,)\n"
     ]
    }
   ],
   "source": [
    "# Train on X_mfcc_random_crop_train (7500),\n",
    "# Test on X_mfcc_test (250)\n",
    "print('Train on padded random crops, validate and test on uncropped test\\n')\n",
    "\n",
    "Xmr_train, _, _ = dl.train_test_val_split(dl.X_mfcc_random_crop, is_cropped=True)\n",
    "Xcr_train, _, _ = dl.train_test_val_split(dl.X_chroma_random_crop, is_cropped=True)\n",
    "\n",
    "Xr_train = np.concatenate([Xmr_train, Xcr_train], axis=1)\n",
    "\n",
    "Y_crop_train, _, _ = dl.train_test_val_split(dl.Y_crop, is_cropped=True)\n",
    "\n",
    "print(f'Train size:   {Xr_train.shape}', f'    Train labels size: {Y_crop_train.shape}')\n",
    "print(f'Val size:     {X_val.shape}', f'     Val labels size:   {Y_val.shape}')\n",
    "print(f'Test size:    {X_test.shape}', f'     Test labels size:  {Y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making torch style dataset and dataloader\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class GenreDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, X, Y = None):\n",
    "        self.X = torch.from_numpy(X).unsqueeze(1)\n",
    "        if type(Y) != type(None):\n",
    "            self.Y = torch.from_numpy(Y)\n",
    "        else:\n",
    "            self.Y = None\n",
    "            \n",
    "    def __len__(self):\n",
    "        if type(self.Y) != type(None):\n",
    "            assert(self.X.size()[0] == self.Y.size()[0])\n",
    "        return self.X.size()[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.X[index]\n",
    "        if type(self.Y) != type(None):\n",
    "            label = self.Y[index]\n",
    "        else:\n",
    "            label = 0\n",
    "        return (data, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN + RNN, FC + RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        num_features = params['audio_in_features']\n",
    "        num_filters_1 = params['cnn_filters_1']\n",
    "        ker_1 = params['cnn_kernel_1']\n",
    "        out_features = params['rnn_in_size']\n",
    "        \n",
    "        out_size = num_features - ker_1[0] + 1\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, num_filters_1, ker_1)\n",
    "        self.fc = nn.Linear(num_filters_1*out_size, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        # out.size() = N x out_channels x out_size x time\n",
    "        out = F.relu(out)\n",
    "        # out.size() = N x out_channels x out_size x time\n",
    "        \n",
    "        out = out.permute(0,3,1,2)\n",
    "        # out.size() = N x time x out_channels x out_size\n",
    "        out = out.view(out.size()[0], out.size()[1], -1)\n",
    "        # out.size() = N x time x out_channels*out_size\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        # out.size() = N x time x out_features\n",
    "        out = F.relu(out)\n",
    "        # out.size() = N x time x out_features\n",
    "        \n",
    "        return out\n",
    "\n",
    "class FC(nn.Module):\n",
    "    # currently no batchnorm\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        super(FC, self).__init__()\n",
    "        \n",
    "        self.deep = params['deep']\n",
    "        \n",
    "        fc_1_in = params['audio_in_features']\n",
    "        fc_1_out = params['fc_1_out']\n",
    "        fc_2_out = params['fc_2_out']\n",
    "        if self.deep:\n",
    "            fc_3_out = params['fc_3_out']\n",
    "            fc_4_out = params['fc_4_out']\n",
    "            fc_5_out = params['rnn_in_size']\n",
    "        else:\n",
    "            fc_3_out = params['rnn_in_size']\n",
    "            \n",
    "        \n",
    "        self.fc_1 = nn.Linear(fc_1_in, fc_1_out)\n",
    "        self.bnorm_1 = nn.BatchNorm1d(fc_1_out)\n",
    "        self.fc_2 = nn.Linear(fc_1_out, fc_2_out)\n",
    "        self.bnorm_2 = nn.BatchNorm1d(fc_2_out)\n",
    "        self.fc_3 = nn.Linear(fc_2_out, fc_3_out)\n",
    "        if self.deep:\n",
    "            self.fc_4 = nn.Linear(fc_3_out, fc_4_out)\n",
    "            self.fc_5 = nn.Linear(fc_4_out, fc_5_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = x.permute(0, 3, 1, 2)\n",
    "        # out.size() = N x time x 1 x fc_1_in\n",
    "        \n",
    "        out = out.view(out.size()[0], out.size()[1], -1)\n",
    "        # out.size() = N x time x out_size\n",
    "        \n",
    "        out = self.fc_1(out)\n",
    "        # out.size() = N x time x fc_1_out\n",
    "        out = out.permute(0,2,1)\n",
    "        # out.size() = N x fc_1_out x time\n",
    "        if self.deep:\n",
    "            out = self.bnorm_1(out)\n",
    "            # out.size() = N x fc_1_out x time\n",
    "        out = out.permute(0,2,1)\n",
    "        # out.size() = N x time x fc_1_out\n",
    "        out = F.relu(out)\n",
    "        # out.size() = N x time x fc_1_out\n",
    "        \n",
    "        out = self.fc_2(out)\n",
    "        # out.size() = N x time x fc_1_out\n",
    "        out = out.permute(0,2,1)\n",
    "        # out.size() = N x fc_2_out x time\n",
    "        # out = self.bnorm_2(out)\n",
    "        # out.size() = N x fc_2_out x time\n",
    "        out = out.permute(0,2,1)\n",
    "        # out.size() = N x time x fc_2_out\n",
    "        out = F.relu(out)\n",
    "        # out.size() = N x time x fc_2_out\n",
    "        \n",
    "        out = self.fc_3(out)\n",
    "        # out.size() = N x time x fc_3_out\n",
    "        out = F.relu(out)\n",
    "        # out.size() = N x time x fc_3_out\n",
    "        \n",
    "        if self.deep:\n",
    "            out = self.fc_4(out)\n",
    "            # out.size() = N x time x fc_4_out\n",
    "            out = F.relu(out)\n",
    "            # out.size() = N x time x fc_4_out\n",
    "\n",
    "            out = self.fc_5(out)\n",
    "            # out.size() = N x time x fc_5_out\n",
    "            out = F.relu(out)\n",
    "            # out.size() = N x time x fc_5_out\n",
    "\n",
    "        return out\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        in_size = params['rnn_in_size']\n",
    "        hid_size = params['rnn_hid_size']\n",
    "        dropout = params['rnn_dropout']\n",
    "        \n",
    "        self.lstm = nn.LSTM(in_size, hid_size, batch_first=True, dropout = dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, hidden = self.lstm(x)\n",
    "        # out.size() = N x time x hid_size\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying new architecture ideas:\n",
    "\n",
    "class CNN_RNN(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(CNN_RNN, self).__init__()\n",
    "        \n",
    "        out_size = params['rnn_hid_size']\n",
    "        \n",
    "        self.CNN = CNN(params)\n",
    "        self.RNN = RNN(params)\n",
    "        \n",
    "        self.fc = nn.Linear(out_size, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.CNN(x)\n",
    "        # out.size() = N x time x rnn_in_size\n",
    "        \n",
    "        out = self.RNN(out)\n",
    "        # out.size() = N x time x rnn_hid_size\n",
    "        \n",
    "        out = out[:,-1,:].squeeze()\n",
    "        # out.size() = N x rnn_hid_size\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        # out.size() = N x 10\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class FC_RNN(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(FC_RNN, self).__init__()\n",
    "        \n",
    "        out_size = params['rnn_hid_size']\n",
    "        \n",
    "        self.FC = FC(params)\n",
    "        self.RNN = RNN(params)\n",
    "        \n",
    "        self.fc = nn.Linear(out_size, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.FC(x)\n",
    "        # out.size() = N x time x fc_3_out\n",
    "        \n",
    "        out = self.RNN(out)\n",
    "        # out.size() = N x time x out_features\n",
    "        \n",
    "        out = out[:,-1,:].squeeze()\n",
    "        # out.size() = N x out_features\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        # out.size() = N x 10\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining training and testing functions\n",
    "\n",
    "def train_single_epoch(net, criterion, optimizer, trainloader, device, silence = False):\n",
    "    # assumes net.train() called\n",
    "    # assumes device is correct\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data in trainloader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        _ , predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += labels.size(0)            \n",
    "\n",
    "        correct += (predicted == labels).sum().item()            \n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()            \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    acc = (100 * correct / total)\n",
    "    avg_loss = running_loss / total\n",
    "    \n",
    "    if not silence:\n",
    "        print(\"Loss: \" + str(running_loss / 750.0) + ' Accuracy: ' + str(acc) + '%')\n",
    "    return avg_loss, acc\n",
    "\n",
    "def test(net, criterion, testloader, device, silence = False):\n",
    "    # assumes net.eval() called\n",
    "    # assumes device is correct\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device).float()\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "    acc = 100*correct/total\n",
    "    avg_loss = running_loss / total\n",
    "    \n",
    "    if not silence:\n",
    "        print('Test accuracy of the network: ' + str(acc) + '%')\n",
    "\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, net, criterion, optimizer, trainloader, testloader = None, silence = False):\n",
    "    # inlude testloader if you want to see progression of train vs testing losses as epochs increase\n",
    "    # in case of prediction then we use train without testloader; the model then predicts upon stuff\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        if not silence:\n",
    "            print(\"Epoch \" + str(epoch+1))\n",
    "        \n",
    "        net.train()\n",
    "        train_loss, train_acc = train_single_epoch(net, criterion, optimizer, trainloader, device)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        if testloader != None:\n",
    "            net.eval()\n",
    "            test_loss, test_acc = test(net, criterion, testloader, device)\n",
    "            test_losses.append(test_loss)\n",
    "            test_accs.append(test_acc)\n",
    "            \n",
    "    return train_losses, train_accs, test_losses, test_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 25, 1290) (250, 25, 1290) (250, 25, 1290) (500,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt at regularization\n",
    "\n",
    "class WeightClipper():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, module):\n",
    "        if hasattr(module, 'weight'):\n",
    "            w = module.weight.data\n",
    "            w = w.clamp(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(train_losses, train_accs, test_losses, test_accs):\n",
    "    tested = (len(test_losses) > 0)\n",
    "    \n",
    "    \n",
    "    plt.figure(1)\n",
    "    plt.plot(list(range(1, len(train_losses)+1)), train_losses, '-b', label='Training Loss')\n",
    "    if tested:    \n",
    "        plt.plot(list(range(1, len(train_losses)+1)), test_losses, '-r', label='Testing Loss')\n",
    "        plt.title('Training and Testing Loss')\n",
    "    else:\n",
    "        plt.title('Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.figure(2)\n",
    "    plt.plot(list(range(1, len(train_losses)+1)), train_accs, '-b', label='Training Accuracy')\n",
    "    if tested:\n",
    "        plt.plot(list(range(1, len(train_losses)+1)), test_accs, '-r', label='Testing Accuracy')\n",
    "        plt.title('Training and Testing Accuracy')\n",
    "    else:\n",
    "        plt.title('Training Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def find_best_stopping_point(test_losses, test_accs):\n",
    "    \n",
    "    best_loss = min(test_losses)\n",
    "    best_loss_index = test_losses.index(best_loss)\n",
    "    \n",
    "    best_acc = max(test_accs)\n",
    "    best_acc_index = test_accs.index(best_acc)\n",
    "    \n",
    "    print(\"Best loss of \" + str(best_loss) +\" after \" + str(best_loss_index+1) + \" epochs of training\")\n",
    "    print(\"Best accuracy of \" + str(best_acc) +\" after \" + str(best_acc_index+1) + \" epochs of training\")\n",
    "    \n",
    "    return (best_loss_index+1, best_acc_index+1) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cr(params, x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    # handling data; creating torch datasets and torch dataloaders\n",
    "    \n",
    "    trainset = GenreDataset(x_train, y_train)\n",
    "    testset = GenreDataset(x_test, y_test)\n",
    "    \n",
    "    trainloader = data.DataLoader(trainset, batch_size = 64, shuffle = True)\n",
    "    testloader = data.DataLoader(testset, batch_size = 64, shuffle = False)\n",
    "    \n",
    "    # constructing net and surrounding items\n",
    "    \n",
    "    cr_net = CNN_RNN(params)\n",
    "    cr_net.float()\n",
    "    clipper = WeightClipper()\n",
    "    cr_net.apply(clipper)\n",
    "    cr_crit = nn.CrossEntropyLoss()\n",
    "    cr_opt = torch.optim.Adam(cr_net.parameters(), lr=params['lr'], weight_decay = params['weight_decay'])\n",
    "    num_epochs = params['epochs']\n",
    "\n",
    "    train_losses, train_accs, test_losses, test_accs = train(num_epochs, cr_net, cr_crit, cr_opt, trainloader, testloader, silence = False)\n",
    "    \n",
    "    # optional graphing with matplotlib\n",
    "    \n",
    "    plot_learning_curve(train_losses, train_accs, test_losses, test_accs)\n",
    "    loss_epochs, _ = find_best_stopping_point(test_losses, test_accs)\n",
    "    return loss_epochs\n",
    "\n",
    "def test_fr(params, x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    # handling data; creating torch datasets and torch dataloaders\n",
    "    \n",
    "    trainset = GenreDataset(x_train, y_train)\n",
    "    testset = GenreDataset(x_test, y_test)\n",
    "    \n",
    "    trainloader = data.DataLoader(trainset, batch_size = 64, shuffle = True)\n",
    "    testloader = data.DataLoader(testset, batch_size = 64, shuffle = False)\n",
    "    \n",
    "    # constructing net and surrounding items\n",
    "    \n",
    "    fr_net = FC_RNN(params)\n",
    "    fr_net.float()\n",
    "    clipper = WeightClipper()\n",
    "    fr_net.apply(clipper)\n",
    "    fr_crit = nn.CrossEntropyLoss()\n",
    "    fr_opt = torch.optim.Adam(fr_net.parameters(), lr=params['lr'], weight_decay = params['weight_decay'])\n",
    "    num_epochs = params['epochs']\n",
    "    silence = \n",
    "\n",
    "    train_losses, train_accs, test_losses, test_accs = train(num_epochs, fr_net, fr_crit, fr_opt, trainloader, testloader, silence = False)\n",
    "    \n",
    "    # optional graphing with matplotlib\n",
    "    \n",
    "    plot_learning_curve(train_losses, train_accs, test_losses, test_accs)\n",
    "    loss_epochs, _ = find_best_stopping_point(test_losses, test_accs)\n",
    "    return loss_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_test_architecture(net_class, params, x_train, y_train, x_test, y_test):\n",
    "    # net_class = CNN_RNN or FC_RNN\n",
    "    \n",
    "    # handling data; creating torch datasets and torch dataloaders\n",
    "    \n",
    "    trainset = GenreDataset(x_train, y_train)\n",
    "    testset = GenreDataset(x_test, y_test)\n",
    "    \n",
    "    trainloader = data.DataLoader(trainset, batch_size = 64, shuffle = True)\n",
    "    testloader = data.DataLoader(testset, batch_size = 64, shuffle = False)\n",
    "    \n",
    "    # constructing net and surrounding items\n",
    "    \n",
    "    params['audio_in_features'] = x_train.shape[1]\n",
    "    \n",
    "    net = net_class(params)\n",
    "    net.float()\n",
    "    clipper = WeightClipper()\n",
    "    net.apply(clipper)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=params['lr'], weight_decay = params['weight_decay'])\n",
    "    num_epochs = params['epochs']\n",
    "    silence = params['silence']\n",
    "\n",
    "    train_losses, train_accs, test_losses, test_accs = train(num_epochs, net, crit, opt, trainloader, silence = silence)\n",
    "    \n",
    "    # displaying learning curve\n",
    "    \n",
    "    if not silence:\n",
    "        plot_learning_curve(train_losses, train_accs, test_losses, test_accs)\n",
    "    \n",
    "    # generating outputs\n",
    "    \n",
    "    val_predictions = predictions(net, valloader)\n",
    "    test_predictions = predictions(net, testloader)\n",
    "\n",
    "    return val_predictions, test_predictions, net # in case we want to save the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after this you're DONE with architectures that's IT\n",
    "# todo: reorganize code\n",
    "# make it so that model is created outside of this whole thing\n",
    "# make a way to save the model and validation/test outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(net, loader):\n",
    "    # return numpy array of model's predictions\n",
    "    \n",
    "    separate_predictions = []\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            \n",
    "            inputs, _ = data\n",
    "            inputs = inputs.to(device).float()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            _ , predicted = torch.max(outputs.data, 1)\n",
    "            separate_predictions.append(predicted.numpy())\n",
    "    \n",
    "    predictions = np.concatenate(separate_predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_evaluate(net_class, params, x_train, y_train, x_val, x_test):\n",
    "    # net_class = CNN_RNN or FC_RNN\n",
    "    \n",
    "    # handling data; creating torch datasets and torch dataloaders\n",
    "    \n",
    "    trainset = GenreDataset(x_train, y_train)\n",
    "    valset = GenreDataset(x_val)\n",
    "    testset = GenreDataset(x_test)\n",
    "    \n",
    "    trainloader = data.DataLoader(trainset, batch_size = 64, shuffle = True)\n",
    "    valloader = data.DataLoader(valset, batch_size = 64, shuffle = False)\n",
    "    testloader = data.DataLoader(testset, batch_size = 64, shuffle = False)\n",
    "    \n",
    "    # constructing net and surrounding items\n",
    "    \n",
    "    params['audio_in_features'] = x_train.shape[1]\n",
    "    \n",
    "    net = net_class(params)\n",
    "    net.float()\n",
    "    clipper = WeightClipper()\n",
    "    net.apply(clipper)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=params['lr'], weight_decay = params['weight_decay'])\n",
    "    num_epochs = params['epochs']\n",
    "    silence = params['silence']\n",
    "\n",
    "    train_losses, train_accs, test_losses, test_accs = train(num_epochs, net, crit, opt, trainloader, silence = silence)\n",
    "    \n",
    "    # displaying learning curve\n",
    "    \n",
    "    if not silence:\n",
    "        plot_learning_curve(train_losses, train_accs, test_losses, test_accs)\n",
    "    \n",
    "    # generating outputs\n",
    "    \n",
    "    val_predictions = predictions(net, valloader)\n",
    "    test_predictions = predictions(net, testloader)\n",
    "\n",
    "    return val_predictions, test_predictions, net # in case we want to save the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THESE ARE THE FINAL PARAMETERS\n",
    "\n",
    "CNN_RNN_PARAMS = {\n",
    "    'audio_in_features' : 25, # static\n",
    "    'cnn_filters_1' : 32, # 32 better than 16 and 48\n",
    "    'cnn_kernel_1' : (16, 1), # 15 better than 10 and 20\n",
    "    'rnn_in_size' : 24,\n",
    "    'rnn_hid_size' : 48,\n",
    "    'rnn_dropout' : 0.5, # seems to do fairly well\n",
    "    'epochs' : 75, # probably won't change\n",
    "    'lr' : 0.01,\n",
    "    'weight_decay': 0.01,\n",
    "    \n",
    "    \n",
    "    'silence' : False\n",
    "}\n",
    "\n",
    "FC_RNN_PARAMS_1 = {\n",
    "    'audio_in_features' : 25, # static\n",
    "    'fc_1_out' : 320, # new\n",
    "    'fc_2_out' : 160, # new\n",
    "    'fc_3_out' : 80, # new, used to be fc_3_out = rnn_in_size = 24\n",
    "    'fc_4_out' : 40, # new\n",
    "    'rnn_in_size' : 24, # probably okay but will investigate\n",
    "    'rnn_hid_size' : 48, # probably okay \n",
    "    'rnn_dropout' : 0.5, # okay\n",
    "    'epochs' : 75, # okay\n",
    "    'lr' : 0.001, # seems like it works well enough\n",
    "    'weight_decay': 0, # seems to not really have an affect\n",
    "    'deep' : False,\n",
    "    \n",
    "    \n",
    "    'silence' : False\n",
    "    # using 1 bnorm layer\n",
    "}\n",
    "\n",
    "FC_RNN_PARAMS_2 = {\n",
    "    'audio_in_features' : 25, # static\n",
    "    'fc_1_out' : 320, # new\n",
    "    'fc_2_out' : 160, # new\n",
    "    'fc_3_out' : 80, # new, used to be fc_3_out = rnn_in_size = 24\n",
    "    'fc_4_out' : 40, # new\n",
    "    'rnn_in_size' : 24, # probably okay but will investigate\n",
    "    'rnn_hid_size' : 48, # probably okay \n",
    "    'rnn_dropout' : 0.5, # okay\n",
    "    'epochs' : 75, # okay\n",
    "    'lr' : 0.001, # seems like it works well enough\n",
    "    'weight_decay': 0, # seems to not really have an affect\n",
    "    'deep' : True,\n",
    "    \n",
    "    \n",
    "    'silence' : False\n",
    "    # using 1 bnorm layer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.39 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# cnn_rnn_val, cnn_rnn_test, cnn_rnn_model = nn_evaluate(CNN_RNN, CNN_RNN_PARAMS, X_train, Y_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# shallow_fc_rnn_val, shallow_fc_rnn_test, shallow_fc_rnn_model = nn_evaluate(FC_RNN, FC_RNN_PARAMS_1, X_train, Y_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.15 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# deep_fc_rnn_val, deep_fc_rnn_test, deep_fc_rnn_model = nn_evaluate(FC_RNN, FC_RNN_PARAMS_2, X_train, Y_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_analysis(x_train, y_train, x_val, x_test, architecture, names = None):\n",
    "    \n",
    "    assert(architecture in ['cnn_rnn, shallow_fc_rnn, deep_fc_rnn'])\n",
    "    \n",
    "    if names == None:\n",
    "        names = {\n",
    "            'val_predictions' : 'val_predictions',\n",
    "            'test_predictions' : 'test_predictions',\n",
    "            'model' : 'model',\n",
    "        }\n",
    "    \n",
    "    if architecture == 'cnn_rnn':\n",
    "        val_predictions, test_predictions, model = nn_evaluate(CNN_RNN, CNN_RNN_PARAMS, x_train, y_train, x_val, x_test)\n",
    "    elif architecture == 'shallow_fc_rnn':\n",
    "        val_predictions, test_predictions, model = nn_evaluate(FC_RNN, FC_RNN_PARAMS_1, x_train, y_train, x_val, x_test)\n",
    "    elif architecture == 'deep_fc_rnn':\n",
    "        val_predictions, test_predictions, model = nn_evaluate(FC_RNN, FC_RNN_PARAMS_2, x_train, y_train, x_val, x_test)\n",
    "    else:\n",
    "        raise ValueError(\"Architecture incorrect and also this is impossible.\")\n",
    "    \n",
    "    # saving outputs\n",
    "    \n",
    "    val_predictions.save('nn/' + names['val_predictions'] + '.npy')\n",
    "    test_predictions.save('nn/' + names['test_predictions'] + '.npy')\n",
    "        \n",
    "    # saving models\n",
    "    \n",
    "    torch.save(model.state_dict(), 'nn/' + names['model'] + '.pth')\n",
    "        \n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nn_analysis(architecture, names):\n",
    "    \n",
    "    val_predictions = np.load('nn/' + names['val_predictions'] + '.npy')\n",
    "    test_predictions = np.load('nn/' + names['test_predictions'] + '.npy')\n",
    "    \n",
    "    if architecture == 'cnn_rnn':\n",
    "        model = CNN_RNN(CNN_RNN_PARAMS)\n",
    "    elif architecture == 'shallow_fc_rnn':\n",
    "        shallow_fc_rnn_model = FC_RNN(FC_RNN_PARAMS_1)\n",
    "    elif architecture == 'deep_fc_rnn':\n",
    "        deep_fc_rnn_model = FC_RNN(FC_RNN_PARAMS_2)\n",
    "    else:\n",
    "        raise ValueError(\"Architecture incorrect and also this is impossible.\")\n",
    "\n",
    "    model.load_state_dict(torch.load('nn/' + names['model'] + '.pth'))\n",
    "\n",
    "    return val_predictions, test_predictions, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_vanilla = {\n",
    "    'val_predictions' : 'val_predictions',\n",
    "    'test_predictions' : 'test_predictions',\n",
    "    'model' : 'model_predictions'\n",
    "}\n",
    "\n",
    "names_fixed_crop = {\n",
    "    'val_predictions' : 'val_predictions_fixed_crop',\n",
    "    'test_predictions' : 'test_predictions_fixed_crop',\n",
    "    'model' : 'model_fixed_crop'\n",
    "}\n",
    "\n",
    "names_random_crop = {\n",
    "    'val_predictions' : 'val_predictions_random_crop',\n",
    "    'test_predictions' : 'test_predictions_random_crop',\n",
    "    'model' : 'model_random_crop'\n",
    "}\n",
    "\n",
    "nn_analysis(X_train, Y_train, X_val, X_test, 'shallow_fc_rnn', names_vanilla)\n",
    "sfr_val, sfr_test, sfr_model = load_nn_analysis('shallow_fc_rnn', names_vanilla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- try different datapreprocessing: change # of channels for 1d stuff so that we get slices through all the different features of the MFCC \n",
    "- rewrite code for DCNN generation to support more versitile dimensionalities\n",
    "- make RNN: LSTM + (D)CNN\n",
    "- uncomment out bnorm in CNN generation code\n",
    "\n",
    "\n",
    "\n",
    "Function:\n",
    "- Input x_train, x_val, x_test, y_train\n",
    "- Output model.predict(x_val), model.predict(x_test)\n",
    "- save model and outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Conv_Block(nn.Module):\n",
    "    \n",
    "#     def __init__(self, in_planes, planes, kernel_size, stride, pool_size):\n",
    "        \n",
    "#         super(Conv_Block, self).__init__()\n",
    "        \n",
    "#         self.conv = nn.Conv1d(in_channels = in_planes, out_channels = planes, kernel_size = kernel_size, stride=stride)\n",
    "#         self.dropout = nn.Dropout(p=0.5)\n",
    "#         self.maxpool = nn.AvgPool1d(kernel_size = pool_size)\n",
    "#         self.bnorm = nn.BatchNorm1d(num_features = planes)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.maxpool(x)\n",
    "#         x = self.bnorm(x)\n",
    "#         x = F.relu(x)\n",
    "        \n",
    "#         return x\n",
    "\n",
    "# class Conv(nn.Module):\n",
    "    \n",
    "#     def __init__(self, params):    \n",
    "#         super(Conv, self).__init__()\n",
    "        \n",
    "#         filters = params['filters']\n",
    "#         kernel_sizes = params['kernel_sizes']\n",
    "#         strides = params['strides']\n",
    "#         avg_pool_sizes = params['max_pool_sizes']\n",
    "        \n",
    "#         assert(len(filters) == len(kernel_sizes))\n",
    "#         assert(len(filters) == len(strides))\n",
    "#         assert(len(filters) == len(avg_pool_sizes))\n",
    "        \n",
    "#         prev_outplanes = 1\n",
    "#         prev_outsize = params['input_length']\n",
    "        \n",
    "#         layers = []\n",
    "        \n",
    "#         for i in range(len(filters)):\n",
    "#             inplanes = prev_outplanes\n",
    "#             outplanes = inplanes * filters[i]\n",
    "#             out_size = math.floor((math.floor((prev_outsize - kernel_sizes[i]) / float(strides[i]))+\\\n",
    "#                                 1)/float(avg_pool_sizes[i]))\n",
    "            \n",
    "#             prev_outsize = out_size\n",
    "#             prev_outplanes = outplanes\n",
    "            \n",
    "#             new_block = Conv_Block(inplanes, outplanes, kernel_sizes[i], strides[i], avg_pool_sizes[i])\n",
    "            \n",
    "#             layers.append(new_block)\n",
    "            \n",
    "#         self.convs = nn.Sequential(*layers)\n",
    "        \n",
    "#         in_features = prev_outsize * prev_outplanes\n",
    "        \n",
    "#         self.fc = nn.Linear(in_features = in_features, out_features = 10)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.convs(x)\n",
    "#         x = x.view(x.size()[0], -1)\n",
    "#         x = self.fc(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging\n",
    "\n",
    "# big big convnet\n",
    "# conv_params = {\n",
    "#     'input_length' : 16770,\n",
    "#     'filters' : [8, 8, 8, 8],\n",
    "#     'kernel_sizes' : [64, 32, 16, 8],\n",
    "#     'strides' : [8, 4, 2, 1],\n",
    "#     'max_pool_sizes' : [2, 2, 2, 2]\n",
    "# }\n",
    "\n",
    "# smaller convnet\n",
    "# conv_params = {\n",
    "#     'input_length' : 16770,\n",
    "#     'filters' : [8, 16, 64],\n",
    "#     'kernel_sizes' : [64, 8, 4],\n",
    "#     'strides' : [8, 4, 2],\n",
    "#     'max_pool_sizes' : [16, 4, 2]\n",
    "# }\n",
    "\n",
    "# smallest convnet\n",
    "conv_params = {\n",
    "    'input_length' : 16770,\n",
    "    'filters' : [8, 64],\n",
    "    'kernel_sizes' : [64, 4],\n",
    "    'strides' : [8, 2],\n",
    "    'max_pool_sizes' : [32, 4]\n",
    "}\n",
    "\n",
    "# observation: this net works as long as we don't run out of elements to convolute across (no padding)\n",
    "\n",
    "conv_test = Conv(conv_params)\n",
    "# clipper = WeightClipper()\n",
    "# conv_test.apply(clipper)\n",
    "summary(conv_test, (1,16770))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DConv_Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_planes, planes, kernel_size, dialation, pool_size):\n",
    "        \n",
    "        super(DConv_Block, self).__init__()\n",
    "        \n",
    "        padding = int((kernel_size + (kernel_size - 1)*(dialation - 1) - 1)/2)\n",
    "        \n",
    "        self.dconv = nn.Conv1d(in_channels = in_planes, out_channels = planes,\n",
    "                               kernel_size = kernel_size, stride=1, padding=padding, dilation=dialation)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.avgpool = nn.AvgPool1d(kernel_size = pool_size)\n",
    "        self.bnorm = nn.BatchNorm1d(num_features = planes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dconv(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.bnorm(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "class DConv(nn.Module):\n",
    "    \n",
    "    def __init__(self, params):    \n",
    "        super(DConv, self).__init__()\n",
    "        \n",
    "        filters = params['filters']\n",
    "        kernel_sizes = params['kernel_sizes']\n",
    "        dialations = params['dialations']\n",
    "        avg_pool_sizes = params['avg_pool_sizes']\n",
    "        \n",
    "        assert(len(filters) == len(kernel_sizes))\n",
    "        assert(len(filters) == len(dialations))\n",
    "        assert(len(filters) == len(avg_pool_sizes))\n",
    "        \n",
    "        prev_outplanes = 1\n",
    "        prev_outsize = params['input_length']\n",
    "\n",
    "        layers = []\n",
    "        \n",
    "        for i in range(len(filters)):\n",
    "            inplanes = prev_outplanes\n",
    "            outplanes = inplanes * filters[i]\n",
    "            \n",
    "            out_size = math.floor(prev_outsize / float(avg_pool_sizes[i]))\n",
    "            \n",
    "            prev_outsize = out_size\n",
    "            prev_outplanes = outplanes\n",
    "            \n",
    "            new_block = DConv_Block(inplanes, outplanes, kernel_sizes[i], dialations[i], avg_pool_sizes[i])\n",
    "            \n",
    "            layers.append(new_block)\n",
    "            \n",
    "        self.dconvs = nn.Sequential(*layers)\n",
    "        \n",
    "        in_features = prev_outsize * prev_outplanes\n",
    "        \n",
    "        self.fc = nn.Linear(in_features = in_features, out_features = 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dconvs(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1            [-1, 16, 16770]           1,040\n",
      "           Dropout-2            [-1, 16, 16770]               0\n",
      "         AvgPool1d-3              [-1, 16, 524]               0\n",
      "       BatchNorm1d-4              [-1, 16, 524]              32\n",
      "       DConv_Block-5              [-1, 16, 524]               0\n",
      "            Conv1d-6              [-1, 64, 524]          16,448\n",
      "           Dropout-7              [-1, 64, 524]               0\n",
      "         AvgPool1d-8              [-1, 64, 131]               0\n",
      "       BatchNorm1d-9              [-1, 64, 131]             128\n",
      "      DConv_Block-10              [-1, 64, 131]               0\n",
      "           Linear-11                   [-1, 10]          83,850\n",
      "================================================================\n",
      "Total params: 101,498\n",
      "Trainable params: 101,498\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 4.99\n",
      "Params size (MB): 0.39\n",
      "Estimated Total Size (MB): 5.44\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Debugging\n",
    "dconv_params = {\n",
    "    'input_length' : 16770,\n",
    "    'filters' : [16, 4],\n",
    "    'kernel_sizes': [64, 16],\n",
    "    'dialations': [8, 2],\n",
    "    'avg_pool_sizes': [32, 4]\n",
    "}\n",
    "\n",
    "# observation: this net works as long as dialation_2 is even\n",
    "\n",
    "dconv_test = DConv(dconv_params)\n",
    "summary(dconv_test, (1,16770))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from brendan\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(test_labels, predictions, title):\n",
    "    ax= plt.subplot()\n",
    "    cm = confusion_matrix(test_labels, predictions)\n",
    "    sns.heatmap(cm, annot=True, ax = ax, cmap = sns.cm.rocket_r); #annot=True to annotate cells\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "    ax.set_title(f'{title} Confusion Matrix'); \n",
    "    ax.set_ylim(top=0, bottom=10)\n",
    "    ax.xaxis.set_ticklabels(all_genres); ax.yaxis.set_ticklabels(all_genres);\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "    plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_labels = Y_test\n",
    "\n",
    "def net_confusion_matrix(net, testloader, test_labels, title):\n",
    "    \n",
    "    separate_predictions = []\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            \n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            separate_predictions.append(predicted.numpy())\n",
    "    \n",
    "    predictions = np.concatenate(separate_predictions)\n",
    "    plot_confusion_matrix(test_labels, predictions, title)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Conv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-199-52b2b29795ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconv_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# regularizing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconv_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Conv' is not defined"
     ]
    }
   ],
   "source": [
    "conv_net = Conv(conv_params)\n",
    "\n",
    "# regularizing\n",
    "conv_net.apply(clipper)\n",
    "\n",
    "conv_crit = nn.CrossEntropyLoss()\n",
    "conv_opt = torch.optim.Adam(conv_net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "dconv_net = DConv(dconv_params)\n",
    "\n",
    "# regularizing\n",
    "clipper = WeightClipper()\n",
    "dconv_net.apply(clipper)\n",
    "\n",
    "dconv_crit = nn.CrossEntropyLoss()\n",
    "dconv_opt = torch.optim.Adam(dconv_net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # training and evaluating CNN\n",
    "\n",
    "# train(conv_net, conv_crit, conv_opt, 30, genre_trainloader)\n",
    "# test(conv_net, genre_testloader)\n",
    "# net_confusion_matrix(conv_net, genre_testloader, Y_test, \"Preliminary CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'genre_trainloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-202-45c0d72f461d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# # training and evaluating DCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdconv_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdconv_crit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdconv_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenre_trainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdconv_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenre_testloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnet_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdconv_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenre_testloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Preliminary DCNN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'genre_trainloader' is not defined"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # training and evaluating DCNN\n",
    "\n",
    "train(dconv_net, dconv_crit, dconv_opt, 30, genre_trainloader)\n",
    "test(dconv_net, genre_testloader)\n",
    "net_confusion_matrix(dconv_net, genre_testloader, Y_test, \"Preliminary DCNN\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
